# 콘서트 대기열 부하 테스트 보고서

---

## 1. 부하 테스트 목적

콘서트 대기열 시스템은 다수의 사용자 요청을 효율적으로 처리해야 합니다. 본 부하 테스트의 주요 목적은 다음과 같습니다:

1. **대기열 토큰 관리**: Redis를 활용한 토큰 발행 및 검증 성능 평가.
2. **좌석 예약 및 결제 처리**: Kafka를 이용한 이벤트 기반 처리 성능 확인.
3. **병목 구간 식별**: 부하 상황에서의 시스템 병목 확인.
4. **확장성 평가**: 시스템의 확장 가능성을 확인하여 향후 트래픽 증가에 대비.

---

## 2. 부하 테스트 대상 시스템

### 대기열 관리

- Redis를 이용해 대기열 토큰 생성 및 상태 관리.
- 사용자 요청에 따라 토큰 발행, 검증 및 만료 처리 수행.

### 좌석 예약 및 결제

- Kafka를 이용한 예약 이벤트 및 결제 이벤트 분리 처리.
- 좌석 상태 변경 및 동시성 문제 방지.

---

## 3. 부하 테스트 계획

### 대기열 관리 부하 테스트

- **목표**: 초당 1,000건의 토큰 발행 및 검증 처리.
- **시나리오**:
  1. 사용자가 대기열 진입 요청.
  2. Redis에서 토큰 생성 및 상태 관리.
  3. 30초 만료 시간을 설정 후 자동 만료.

### 좌석 예약 및 결제 부하 테스트

- **목표**: 초당 500건의 좌석 예약 및 결제 처리.
- **시나리오**:
  1. Kafka Producer가 예약 이벤트 발행.
  2. Kafka Consumer가 이벤트를 처리하여 좌석 예약.
  3. Kafka Producer가 결제 이벤트 발행.
  4. Kafka Consumer가 결제 성공 이벤트 처리 후 좌석 확정.

---

## 4. 테스트 환경 구성

### 서버 구성

- **Application Server**: 4 Core, 8GB RAM, 2 Instances.
- **Redis**: Cluster Mode, 3 Nodes.
- **Kafka**: 3 Brokers, Replication Factor 2.

### 테스트 툴

- **Apache JMeter**: HTTP API 부하 테스트.
- **K6**: Redis 및 Kafka 이벤트 처리 시뮬레이션.

### 부하 테스트 스크립트

- HTTP POST 요청으로 대기열 및 예약 API 호출.
- Kafka Producer 및 Consumer 동작 스크립트 작성.

---

## 5. 테스트 결과

### 대기열 관리 테스트 결과

- **목표**: 초당 1,000건의 토큰 처리.
- **결과**:
  - 평균 응답 시간: 50ms.
  - 최대 처리량: 초당 1,200건.
  - Redis 사용률: 70%.
  - 성공률: 99.8%.
  - 병목 구간: Redis 연결 증가 시 Latency 약간 상승.

### 좌석 예약 및 결제 테스트 결과

- **목표**: 초당 500건의 예약 및 결제 처리.
- **결과**:
  - 평균 응답 시간: 100ms.
  - Kafka Consumer 처리율: 초당 600건.
  - 성공률: 99.5%.
  - 병목 구간: Consumer 스레드 수 부족으로 처리 지연 발생.

---

## 6. 실사용 중 발생할 수 있는 장애 사례

### Redis 장애 시

1. **Redis 서버 다운**:

   - **현상**: 대기열 토큰 발행 및 검증 실패.
   - **원인**: Redis 메모리 초과, 네트워크 단절, 서버 장애.
   - **해결 방안**:
     - Redis Sentinel 설정으로 장애 발생 시 자동 장애 조치.
     - 클러스터 노드 추가로 부하 분산.
     - 만료 데이터를 주기적으로 삭제하여 메모리 관리.

2. **CPU 및 RAM 초과**:
   - **현상**: Latency 급증 및 요청 실패.
   - **원인**: 과도한 요청 처리로 자원 초과.
   - **해결 방안**:
     - Redis의 maxmemory 정책을 활용하여 메모리 초과 시 LRU 데이터 삭제.
     - 캐싱 구조 최적화로 불필요한 데이터 저장 방지.

### Kafka 장애 시

1. **Kafka 브로커 다운**:

   - **현상**: 예약 및 결제 이벤트 처리 중단.
   - **원인**: 브로커의 메모리 또는 디스크 초과, 네트워크 장애.
   - **해결 방안**:
     - Replication Factor 증가로 데이터 유실 방지.
     - 클러스터 크기 확장으로 브로커 장애 시 대체 브로커 사용.

2. **Consumer 처리 지연**:

   - **현상**: 예약 및 결제 처리 속도 감소.
   - **원인**: Consumer 스레드 부족 또는 처리량 초과.
   - **해결 방안**:
     - Consumer 스레드 수 증가로 처리량 확장.
     - Kafka 파티션 수 증가로 병렬 처리 강화.

3. **디스크 용량 초과**:
   - **현상**: 메시지 저장 불가로 브로커 다운.
   - **해결 방안**:
     - 메시지 보존 기간(log.retention.ms) 조정.
     - 불필요한 토픽 및 메시지 삭제.

---

## 7. 결론 및 개선 방안

### 결론

- Redis와 Kafka는 대규모 요청을 처리하기 적합하나, 장애 발생 가능성에 대한 대비 필요.
- 부하 테스트를 통해 병목 구간 및 성능 한계를 파악하여 개선 방향 설정 가능.

### 개선 방안

1. **Redis**:

   - Sentinel 및 클러스터 구성으로 장애 대응.
   - maxmemory 정책 및 만료 데이터 관리로 메모리 최적화.

2. **Kafka**:

   - Replication Factor 및 Consumer 스레드 증가로 안정성 강화.
   - 파티션 수 증가 및 메시지 보존 정책 최적화.

3. **모니터링**:
   - Redis 및 Kafka에 대해 Prometheus와 Grafana를 활용한 실시간 모니터링 설정.
   - 장애 발생 시 알림 시스템 연동.

---
